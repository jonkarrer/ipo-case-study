{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "058ee606",
   "metadata": {},
   "source": [
    "# IPO Prospectus Data Exploration and Analysis\n",
    "\n",
    "## Goal\n",
    "Use the IPO Prospectus data to create a model that predicts whether a company will have a successful IPO or not.\n",
    "\n",
    "## Data\n",
    "\n",
    "Acquired from scraping thousands of SEC prospectus documents. The data was stored in HTML format, and then converted to a CSV file. Things like keywords, underwriting, and financial data were extracted from the HTML and stored in new columns. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7e7a49",
   "metadata": {},
   "source": [
    "## Preprocessing Data\n",
    "\n",
    "Applying the fundamental rules of data cleaning and feature engineering to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "33fc8b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.preprocessing import StandardScaler, MultiLabelBinarizer\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def create_original_dataframe():\n",
    "    original_df = pd.read_csv('../data/all_financial_with_keywords.csv') \n",
    "    original_df = original_df.drop_duplicates()\n",
    "    return original_df\n",
    "\n",
    "original_df = create_original_dataframe()\n",
    "\n",
    "### Prep Target Column\n",
    "def prep_target_variable(original_df, column='diff'):\n",
    "    # Convert to percent based deltas\n",
    "    original_df['open'] = original_df[column] + original_df['close']\n",
    "    original_df['diff_percent'] = original_df[column] / original_df['close']\n",
    "    \n",
    "   # Remove the over 100% change values\n",
    "    final_df = original_df[(original_df['diff_percent'] > -1.0) & (original_df['diff_percent'] < 1.0)]\n",
    "    return final_df\n",
    "\n",
    "original_df = prep_target_variable(original_df)\n",
    "target_column = 'diff_percent'\n",
    "\n",
    "bank_columns = [\n",
    "    'citigroup', 'citi', 'morgan stanley', 'ubs', 'barclays', 'wells fargo', \n",
    "    'goldman sachs', 'deutsche bank', 'credit suisse', 'merrill lynch', \n",
    "    'rbc capital', 'jefferies', 'stifel', 'morgan stanley.1', 'merrill lynch.1', \n",
    "    'goldman sachs.1', 'raymond james', 'piper jaffray', 'robert w. baird', \n",
    "    'william blair', 'william blair.1', 'hill road', 'document_length', 'word_count'\n",
    "]\n",
    "\n",
    "keyword_columns = [\n",
    "    'technology', 'software', 'ai', 'machine learning', 'cloud', 'saas', \n",
    "    'platform', 'digital', 'data', 'analytics', 'algorithm', 'automation', \n",
    "    'blockchain', 'cryptocurrency', 'cybersecurity', 'subscription', 'recurring',\n",
    "    'e-commerce', 'mobile', 'app', 'virtual', 'healthcare', 'biotech', \n",
    "    'pharmaceutical', 'medical', 'clinical', 'energy', 'renewable', 'solar', \n",
    "    'electric', 'battery', 'real estate', 'logistics', 'transportation', \n",
    "    'automotive', 'document_length', 'word_count' \n",
    "]\n",
    "\n",
    "financial_columns = [\n",
    "    'additional_paid_in_capital_trend', 'additional_paid_in_capital_recent',\n",
    "    'total_assets_trend', 'total_assets_recent', 'total_current_liabilities_trend', \n",
    "    'total_current_liabilities_recent', 'total_liabilities_trend', 'total_liabilities_recent', \n",
    "    'cash_trend', 'cash_recent', 'total_capitalization_trend', 'total_capitalization_recent', \n",
    "    'total_current_assets_trend', 'total_current_assets_recent', 'volume', \n",
    "    'close', 'diff', 'price_public_total', 'public_price_per_share_x', \n",
    "    'public_price_per_share_y' \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "80a2e3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def prepare_bank_count_feature(df, columns):\n",
    "    # Make a copy to avoid modifying the original\n",
    "    df = df.copy()\n",
    "    \n",
    "    ### Convert NA to 0\n",
    "    df[columns] = df[columns].fillna(0)\n",
    "\n",
    "    ### Remove Dupes\n",
    "    dupes = [\n",
    "        'morgan stanley.1',\n",
    "        'merrill lynch.1',\n",
    "        'goldman sachs.1',\n",
    "        'william blair.1'\n",
    "    ]\n",
    "\n",
    "    for dupe in dupes:\n",
    "        # Fix: Check if column exists in the subset\n",
    "        if dupe not in df[columns].columns:\n",
    "            continue\n",
    "        root_name = dupe.split('.')[0]\n",
    "        # Fix: Work directly with the main dataframe\n",
    "        df[root_name] = df[root_name] + df[dupe]\n",
    "        df = df.drop(dupe, axis=1)\n",
    "        # Update columns list to reflect dropped column\n",
    "        if dupe in columns:\n",
    "            columns = [col for col in columns if col != dupe]\n",
    "        \n",
    "    ### Feature Engineer Bank Group Column\n",
    "    cols = list(df[columns].columns)\n",
    "    # Fix: Add new column to main dataframe, not subset\n",
    "    df['bank_group'] = df[columns].apply(\n",
    "        lambda row: [col for col, val in row.items() \n",
    "                    if val >= 1 and col != 'word_count' and col != 'document_length'], \n",
    "        axis=1\n",
    "    )\n",
    "    # Encoded value of bank name is its index in the cols list \n",
    "    df['bank_group'] = df['bank_group'].apply(\n",
    "        lambda row: [cols.index(bank_name) for bank_name in row]\n",
    "    )\n",
    "\n",
    "    ### Feature Engineer Unique Bank Count\n",
    "    df['unique_bank_count'] = df['bank_group'].apply(lambda row: len(row))\n",
    "\n",
    "    ### Feature Engineer Bank Count Ratios\n",
    "    for col in df[columns].columns:\n",
    "        if col in ['bank_group', 'word_count', 'document_length', 'unique_bank_count']:\n",
    "            continue\n",
    "        new_col = f'{col}-word-count-ratio'\n",
    "        df[new_col] = df[col] / df['word_count']\n",
    "\n",
    "    ### Feature Engineer Parallel Log1p Column\n",
    "    for col in df[columns].columns:\n",
    "        if col in ['bank_group', 'word_count', 'document_length', 'unique_bank_count'] or 'word-count-ratio' in col:\n",
    "            continue\n",
    "        new_col = f'{col}-log1p'\n",
    "        df[new_col] = np.log1p(df[col])   \n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def prepare_keyword_count_feature(df, columns):\n",
    "    # Make a copy to avoid modifying the original\n",
    "    df = df.copy()\n",
    "    \n",
    "    ### Convert NA to 0\n",
    "    df[columns] = df[columns].fillna(0)\n",
    "        \n",
    "    ### Feature Engineer Keyword Group Column\n",
    "    cols = list(df[columns].columns)\n",
    "    # Remove non-keyword columns, check if column has a value\n",
    "    df['keyword_group'] = df[columns].apply(\n",
    "        lambda row: [col for col, val in row.items() \n",
    "                    if val >= 1 and col != 'word_count' and col != 'document_length'], \n",
    "        axis=1\n",
    "    )\n",
    "    # Encoded value of keyword name is its index in the cols list \n",
    "    df['keyword_group'] = df['keyword_group'].apply(\n",
    "        lambda row: [cols.index(keyword_name) for keyword_name in row]\n",
    "    )\n",
    "\n",
    "    ### Feature Engineer Unique Keyword Count\n",
    "    # Count the length of the keyword_groups array\n",
    "    df['unique_keyword_count'] = df['keyword_group'].apply(lambda row: len(row))\n",
    "\n",
    "    ### Feature Engineer Keyword Count Ratios\n",
    "    for col in df[columns].columns:\n",
    "        if col in ['keyword_group', 'word_count', 'document_length', 'unique_keyword_count']:\n",
    "            continue\n",
    "        new_col = f'{col}-word-count-ratio'\n",
    "        # Fix: Reference the main dataframe for both numerator and denominator\n",
    "        df[new_col] = df[col] / df['word_count']\n",
    "\n",
    "    ### Feature Engineer Parallel Log1p Column\n",
    "    for col in df[columns].columns:\n",
    "        if col in ['keyword_group', 'word_count', 'document_length', 'unique_keyword_count'] or 'word-count-ratio' in col:\n",
    "            continue\n",
    "        new_col = f'{col}-log1p'\n",
    "        # Fix: Reference the main dataframe\n",
    "        df[new_col] = np.log1p(df[col])   \n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def prepare_financial_feature(df, columns):\n",
    "    ### Calculate computed columns\n",
    "    def safe_ratio(numerator, denominator, fill_value=np.nan):\n",
    "        return np.where(denominator != 0, numerator / denominator, fill_value)\n",
    "\n",
    "    # Make a copy to avoid modifying the original\n",
    "    df = df.copy()\n",
    "    \n",
    "    ### Remove dupes and unused columns\n",
    "    # Work directly with the main dataframe for structural changes\n",
    "    df = df.drop(\"public_price_per_share_x\", axis=1, errors=\"ignore\")\n",
    "    df = df.rename(columns={\"public_price_per_share_y\": \"public_price_per_share\"})\n",
    "    df = df.drop(\"close\", axis=1, errors=\"ignore\")\n",
    "    df = df.drop(\"volume\", axis=1, errors=\"ignore\")\n",
    "    \n",
    "    # Update columns list to reflect any dropped columns\n",
    "    columns = [col for col in columns if col in df.columns]\n",
    "    # Add the renamed column if it exists\n",
    "    if \"public_price_per_share\" in df.columns and \"public_price_per_share\" not in columns:\n",
    "        columns.append(\"public_price_per_share\")\n",
    "\n",
    "    ### Normalize\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(df[columns])\n",
    "    # Fix: Assign back to the main dataframe with proper index alignment\n",
    "    df[columns] = pd.DataFrame(scaled_data, columns=columns, index=df.index)\n",
    "\n",
    "    ### Missing data flag\n",
    "    df[columns] = df[columns].fillna(0)\n",
    "    # Fix: Add new column to main dataframe\n",
    "    df['missing_financials'] = (df[columns] == 0).sum(axis=1)\n",
    "    # Note: Removed the .value_counts() as it doesn't assign to anything\n",
    "\n",
    "    ### Calculate change percentages\n",
    "    for col in df[columns].columns:\n",
    "        if col.endswith('_trend'):\n",
    "            first_col = col.replace('_trend', '_first')\n",
    "            recent_col = col.replace('_trend', '_recent')\n",
    "            change_col = col.replace('_trend', '_change')\n",
    "            \n",
    "            # Fix: Add new columns to main dataframe\n",
    "            df[first_col] = df[recent_col] - df[col]\n",
    "            df[change_col] = safe_ratio(df[col], df[first_col])\n",
    "\n",
    "    \n",
    "    # Fix: Add new columns to main dataframe, not subset\n",
    "    df['asset_to_liability_ratio'] = safe_ratio(\n",
    "        df['total_assets_recent'], \n",
    "        df['total_liabilities_recent']\n",
    "    )\n",
    "\n",
    "    df['liability_to_capital_ratio'] = safe_ratio(\n",
    "        df['total_liabilities_recent'], \n",
    "        df['total_capitalization_recent']\n",
    "    )\n",
    "\n",
    "    df['asset_to_capital_ratio'] = safe_ratio(\n",
    "        df['total_assets_recent'], \n",
    "        df['total_capitalization_recent']\n",
    "    )\n",
    "\n",
    "    df['liability_to_assets_ratio'] = safe_ratio(\n",
    "        df['total_liabilities_recent'], \n",
    "        df['total_assets_recent']\n",
    "    )\n",
    "\n",
    "    df['assets_to_public_price_ratio'] = safe_ratio(\n",
    "        df['total_assets_recent'], \n",
    "        df['public_price_per_share']\n",
    "    )\n",
    "\n",
    "    df['liabilities_to_public_price_ratio'] = safe_ratio(\n",
    "        df['total_liabilities_recent'], \n",
    "        df['public_price_per_share']\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "cleaned_df = prepare_bank_count_feature(original_df, bank_columns)\n",
    "cleaned_df = prepare_keyword_count_feature(cleaned_df, keyword_columns)\n",
    "cleaned_df = prepare_financial_feature(cleaned_df, financial_columns)\n",
    "\n",
    "## Convert Dates to numeric\n",
    "cleaned_df['ipo_date'] = pd.to_datetime(cleaned_df['ipo_date'])\n",
    "cleaned_df['ipo_day'] = cleaned_df['ipo_date'].dt.day\n",
    "cleaned_df['ipo_month'] = cleaned_df['ipo_date'].dt.month\n",
    "cleaned_df['ipo_year'] = cleaned_df['ipo_date'].dt.year\n",
    "\n",
    "### Drop String columns\n",
    "cleaned_df.drop('symbol', axis=1, inplace=True, errors='ignore')\n",
    "cleaned_df.drop('url', axis=1, inplace=True, errors='ignore')\n",
    "cleaned_df.drop('ipo_date', axis=1, inplace=True, errors='ignore')\n",
    "cleaned_df.drop('day', axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "## Transform to one-hot encoded\n",
    "mlb_bank = MultiLabelBinarizer()\n",
    "mlb_keyword = MultiLabelBinarizer()\n",
    "\n",
    "# Encode bank_group\n",
    "encoded_bank_group = mlb_bank.fit_transform(cleaned_df['bank_group'])\n",
    "bank_df = pd.DataFrame(\n",
    "    encoded_bank_group,\n",
    "    columns=[f'bank_group_{group}' for group in mlb_bank.classes_],\n",
    "    index=cleaned_df.index\n",
    ")\n",
    "\n",
    "# Encode keyword_group\n",
    "encoded_keyword_group = mlb_keyword.fit_transform(cleaned_df['keyword_group'])\n",
    "keyword_df = pd.DataFrame(\n",
    "    encoded_keyword_group,\n",
    "    columns=[f'keyword_group_{group}' for group in mlb_keyword.classes_],\n",
    "    index=cleaned_df.index\n",
    ")\n",
    "\n",
    "# Drop original array columns\n",
    "cleaned_df.drop(['bank_group', 'keyword_group'], axis=1, inplace=True)\n",
    "\n",
    "# Concatenate all dataframes\n",
    "cleaned_df = pd.concat([cleaned_df, bank_df, keyword_df], axis=1)\n",
    "\n",
    "cleaned_df.to_csv('../data/processed_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e67984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>additional_paid_in_capital_trend</th>\n",
       "      <th>additional_paid_in_capital_recent</th>\n",
       "      <th>total_assets_trend</th>\n",
       "      <th>total_assets_recent</th>\n",
       "      <th>total_current_liabilities_trend</th>\n",
       "      <th>total_current_liabilities_recent</th>\n",
       "      <th>total_liabilities_trend</th>\n",
       "      <th>total_liabilities_recent</th>\n",
       "      <th>cash_trend</th>\n",
       "      <th>cash_recent</th>\n",
       "      <th>...</th>\n",
       "      <th>keyword_group_25</th>\n",
       "      <th>keyword_group_26</th>\n",
       "      <th>keyword_group_27</th>\n",
       "      <th>keyword_group_28</th>\n",
       "      <th>keyword_group_29</th>\n",
       "      <th>keyword_group_30</th>\n",
       "      <th>keyword_group_31</th>\n",
       "      <th>keyword_group_32</th>\n",
       "      <th>keyword_group_33</th>\n",
       "      <th>keyword_group_34</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.065148</td>\n",
       "      <td>-0.143178</td>\n",
       "      <td>-0.070095</td>\n",
       "      <td>-0.090368</td>\n",
       "      <td>-0.040564</td>\n",
       "      <td>-0.053970</td>\n",
       "      <td>-0.044359</td>\n",
       "      <td>-0.063151</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.024413</td>\n",
       "      <td>-0.106855</td>\n",
       "      <td>-0.069812</td>\n",
       "      <td>-0.090995</td>\n",
       "      <td>-0.039780</td>\n",
       "      <td>-0.055513</td>\n",
       "      <td>-0.044359</td>\n",
       "      <td>-0.063882</td>\n",
       "      <td>-0.044771</td>\n",
       "      <td>-0.077408</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.024413</td>\n",
       "      <td>-0.106855</td>\n",
       "      <td>-0.069812</td>\n",
       "      <td>-0.090995</td>\n",
       "      <td>-0.039780</td>\n",
       "      <td>-0.055513</td>\n",
       "      <td>-0.044359</td>\n",
       "      <td>-0.063882</td>\n",
       "      <td>-0.044771</td>\n",
       "      <td>-0.077408</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.064021</td>\n",
       "      <td>-0.142304</td>\n",
       "      <td>-0.069993</td>\n",
       "      <td>-0.091047</td>\n",
       "      <td>-0.040536</td>\n",
       "      <td>-0.055835</td>\n",
       "      <td>-0.044342</td>\n",
       "      <td>-0.064012</td>\n",
       "      <td>-0.045131</td>\n",
       "      <td>-0.077720</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.065148</td>\n",
       "      <td>-0.106871</td>\n",
       "      <td>-0.070095</td>\n",
       "      <td>-0.091165</td>\n",
       "      <td>-0.040564</td>\n",
       "      <td>-0.055926</td>\n",
       "      <td>-0.044359</td>\n",
       "      <td>-0.038477</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 261 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   additional_paid_in_capital_trend  additional_paid_in_capital_recent  \\\n",
       "0                         -0.065148                          -0.143178   \n",
       "1                         -0.024413                          -0.106855   \n",
       "2                         -0.024413                          -0.106855   \n",
       "3                         -0.064021                          -0.142304   \n",
       "4                         -0.065148                          -0.106871   \n",
       "\n",
       "   total_assets_trend  total_assets_recent  total_current_liabilities_trend  \\\n",
       "0           -0.070095            -0.090368                        -0.040564   \n",
       "1           -0.069812            -0.090995                        -0.039780   \n",
       "2           -0.069812            -0.090995                        -0.039780   \n",
       "3           -0.069993            -0.091047                        -0.040536   \n",
       "4           -0.070095            -0.091165                        -0.040564   \n",
       "\n",
       "   total_current_liabilities_recent  total_liabilities_trend  \\\n",
       "0                         -0.053970                -0.044359   \n",
       "1                         -0.055513                -0.044359   \n",
       "2                         -0.055513                -0.044359   \n",
       "3                         -0.055835                -0.044342   \n",
       "4                         -0.055926                -0.044359   \n",
       "\n",
       "   total_liabilities_recent  cash_trend  cash_recent  ...  keyword_group_25  \\\n",
       "0                 -0.063151    0.000000     0.000000  ...                 0   \n",
       "1                 -0.063882   -0.044771    -0.077408  ...                 0   \n",
       "2                 -0.063882   -0.044771    -0.077408  ...                 0   \n",
       "3                 -0.064012   -0.045131    -0.077720  ...                 0   \n",
       "4                 -0.038477    0.000000     0.000000  ...                 0   \n",
       "\n",
       "   keyword_group_26  keyword_group_27  keyword_group_28  keyword_group_29  \\\n",
       "0                 1                 0                 0                 1   \n",
       "1                 0                 0                 0                 0   \n",
       "2                 0                 0                 0                 0   \n",
       "3                 1                 1                 0                 1   \n",
       "4                 1                 0                 0                 1   \n",
       "\n",
       "   keyword_group_30  keyword_group_31  keyword_group_32  keyword_group_33  \\\n",
       "0                 0                 1                 0                 0   \n",
       "1                 0                 1                 0                 0   \n",
       "2                 0                 1                 0                 0   \n",
       "3                 1                 1                 0                 1   \n",
       "4                 1                 1                 0                 1   \n",
       "\n",
       "   keyword_group_34  \n",
       "0                 1  \n",
       "1                 1  \n",
       "2                 1  \n",
       "3                 0  \n",
       "4                 1  \n",
       "\n",
       "[5 rows x 261 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/processed_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d134eba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5ed612",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2b5ffa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-ipo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
